{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNNCode.ipynb","provenance":[{"file_id":"1_Y_zu5HkzWHwWmvYCx1m3zGWxXdveLAF","timestamp":1599666717360},{"file_id":"https://github.com/Manikanta-Munnangi/CROP---Plant-Disease-Identification-Using-App/blob/master/Cnn-Code/Crop.ipynb","timestamp":1599393755985}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"kTdhE8cZ_jJW"},"source":["# `Mounting Dataset From Drive.` "]},{"cell_type":"code","metadata":{"id":"RVCGkdwogKPt"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MWRqUXiKC-Cy"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"7t4rcphqCqK1"},"source":["# `1.Import Libraries.`"]},{"cell_type":"code","metadata":{"id":"o9XcN_vmg38P"},"source":["# Import Libraries\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import os\n","import glob\n","import matplotlib.pyplot as plt\n","\n","import tensorflow as tf\n","print(tf.__version__)\n","\n","# Keras API\n","import keras\n","from keras.models import Sequential\n","from keras.layers import Dense,Dropout,Flatten\n","from keras.layers import Conv2D,MaxPooling2D,Activation,AveragePooling2D,BatchNormalization\n","from keras.preprocessing.image import ImageDataGenerator"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8EJ2OkcwC-xy"},"source":["# `2.Load Data into Train and Test Variables.`"]},{"cell_type":"code","metadata":{"id":"3KoUJXj9ik2_"},"source":["# My data is in google drive.\n","train_dir =\"drive/My Drive/Train/\"\n","test_dir=\"drive/My Drive/Test/\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dPfOAeA8DZ9b"},"source":["# `3.Function To count Images In Each Data Set.`"]},{"cell_type":"code","metadata":{"id":"CGx5-TibjLMn"},"source":["# function to get count of images\n","def get_files(directory):\n","  if not os.path.exists(directory):\n","    return 0\n","  count=0\n","  for current_path,dirs,files in os.walk(directory):\n","    for dr in dirs:\n","      count+= len(glob.glob(os.path.join(current_path,dr+\"/*\")))\n","  return count"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d-CTlyJ_usrY"},"source":["# Preprocessing data.\n","train_datagen = ImageDataGenerator( rescale=1./255,\n","                                   shear_range=0.2,\n","                                   zoom_range=0.2, # Random zoom\n","                                   validation_split=0.2, # validation split 20%.\n","                                   horizontal_flip=True)\n"," \n","test_datagen = ImageDataGenerator(rescale=1./255)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EIffvBpe-y8N"},"source":["# `4.CNN Parameter Building.`"]},{"cell_type":"code","metadata":{"id":"uE1cHgPijwx1"},"source":["train_samples = get_files(train_dir)\n"," \n","num_classes = len(glob.glob(train_dir+\"/*\"))\n"," \n","test_samples=get_files(test_dir) \n"," \n","print(num_classes,\"Classes\")\n","print(train_samples,\"Train images\")\n","print(test_samples,\"Test images\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"02pX54IExHAI"},"source":["# CNN building.\n","model = Sequential()\n"," \n","model.add(Conv2D(32, (5, 5),input_shape=input_shape,activation='relu'))\n","model.add(MaxPooling2D(pool_size=(3, 3)))\n"," \n","model.add(Conv2D(32, (3, 3),activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n"," \n","model.add(Conv2D(64, (3, 3),activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))   \n"," \n","model.add(Flatten()) # Flatten \n"," \n","model.add(Dense(512,activation='relu'))\n","model.add(Dropout(0.25))\n"," \n","model.add(Dense(128,activation='relu'))          \n","model.add(Dense(num_classes,activation='softmax'))\n"," \n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H8NqDpMysS_K"},"source":["# set height and width and color of input image.\n","img_width,img_height =256,256\n","input_shape=(img_width,img_height,3)\n","batch_size =32\n"," \n","train_generator =train_datagen.flow_from_directory(train_dir,\n","                                                   target_size=(img_width,img_height),\n","                                                   batch_size=batch_size)\n"," \n","test_generator=test_d hiatagen.flow_from_directory(test_dir,shuffle=True,\n","                                                   target_size=(img_width,img_height),\n","                                                   batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MR5A-tIKR8oY"},"source":["model_layers = [ layer.name for layer in model.layers]\n","print('layer name : ',model_layers)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"txZYUHCYAnbm"},"source":["# `6.Training The Model.`"]},{"cell_type":"code","metadata":{"id":"qv_ow-jPBDnQ"},"source":["# validation data.\n","validation_generator = train_datagen.flow_from_directory(\n","    train_dir, # same directory as training data\n","    target_size=(img_height, img_width),\n","    batch_size=batch_size) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d1GPempn06uE"},"source":["# Model building to get trained with parameters.\n","opt=keras.optimizers.Adam(lr=0.001)\n","model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])\n","train = model.fit_generator(train_generator,\n","                          epochs=15,\n","                          steps_per_epoch=train_generator.samples // batch_size,\n","                          validation_data=validation_generator,\n","                          validation_steps= validation_generator.samples// batch_size,verbose=1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AYpo1Sr5AXv-"},"source":["# `7.Plot For Accuracy And Losses.`"]},{"cell_type":"code","metadata":{"id":"O_SLJxPxvORf"},"source":["acc = train.history['acc']\n","val_acc = train.history['val_acc']\n","loss = train.history['loss']\n","val_loss = train.history['val_loss']\n","epochs = range(1, len(acc) + 1)\n","#Train and validation accuracy \n","plt.plot(epochs, acc, 'b', label='Training accurarcy')\n","plt.plot(epochs, val_acc, 'r', label='Validation accurarcy')\n","plt.title('Training and Validation accurarcy')\n","plt.legend()\n"," \n","plt.figure()\n","#Train and validation loss\n","plt.plot(epochs, loss, 'b', label='Training loss')\n","plt.plot(epochs, val_loss, 'r', label='Validation loss')\n","plt.title('Training and Validation loss')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T2qQ20VwUPvO"},"source":["# `8. Evaluate model using unseen data.`\n"]},{"cell_type":"code","metadata":{"id":"sWsPrcGkWBYC"},"source":["score,accuracy =model.evaluate(test_generator,verbose=1)\n","print(\"Test score is {}\".format(score))\n","print(\"Test accuracy is {}\".format(accuracy))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BYs8zwNeAJE-"},"source":["# `9.Saving Model.` "]},{"cell_type":"code","metadata":{"id":"xZuavjSmz6wb"},"source":["# Save entire model with optimizer, architecture and training configuration.\n","from keras.models import load_model\n","model.save('seed.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SwbrCZY8_41b"},"source":["# `10.Load Model.`"]},{"cell_type":"code","metadata":{"id":"pFeWlgy_5oSy"},"source":["# Loading model and predict.\n","from keras.models import load_model\n","model=load_model('seed.h5')\n"," \n","Classes = [\"good\",\"bad\",\"worst\",\"excellent\",\"average\",\"good\",\"bad\",\"worst\",\"excellent\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EkPDy01Pn9C2"},"source":["good\n","bad\n","worst\n","excellent\n","average\n","Good\n","Bad\n","Worst\n","Excellent\n","Average"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u3w5ndzU_Qqi"},"source":["# `11.Time For Predictions.`"]},{"cell_type":"code","metadata":{"id":"7Crovs5tgVpx"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Pre-Processing test data same as train data.\n","img_width=256\n","img_height=256\n","#model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n","\n","from keras.preprocessing import image\n","\n","def prepare(img_path):\n","    img = image.load_img(img_path, target_size=(256, 256))\n","    x = image.img_to_array(img)\n","    x = x/255\n","    return np.expand_dims(x, axis=0)\n","    \n","    \n","result = model.predict_classes([prepare('/content/drive/My Drive/Test/seed/dc09c4e1-8b58-40e4-a2f2-61b3f19eff94___Matt.S_CG 1010.JPG')])\n","disease=image.load_img('/content/drive/My Drive/Test/seed/dc09c4e1-8b58-40e4-a2f2-61b3f19eff94___Matt.S_CG 1010.JPG')\n","plt.imshow(disease)\n","print (Classes[int(result)])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EXsCoNhv8_oE"},"source":["# `12.Convert Model To \"tflite format.\"`.\n","- This conversion is done because to make our model interpertable with App.\n","- tflite is tensorflowlite made for mobile versions."]},{"cell_type":"code","metadata":{"id":"9oaFo9BH-Bgh"},"source":["import tensorflow as tf\n","converter = tf.lite.TFLiteConverter.from_keras_model_file( 'seed.h5' )\n","model = converter.convert()\n","file = open( 'model.tflite' , 'wb' )\n","file.write( model )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iYmsE2Qspq8s"},"source":[""]}]}